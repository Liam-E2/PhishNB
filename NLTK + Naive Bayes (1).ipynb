{"cells":[{"metadata":{},"cell_type":"markdown","source":"Naive Bayes Classification for Phish Show Reviews V1\n\nTODO:\n* Scrape Summer 2019 Data to compare\n* Ensemble of reviews per show?\n* Different Models?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport pandas as pd\nimport os \nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n\nN_MC = 2000\nCUTOFF_PROB = 0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Phase 1: Load Data and Process text\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/phish-reviews/Reviews.csv\")\ndf.head()\ndf = df[df['Unnamed: 0'] > 829]\ndf = df[df['Unnamed: 0'] < 4855]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regex to remove punctuation\ndef remove_punctuation(review):\n    symbols=\"[â€™'`!)(&@#.,/'`~:;|\\?]\"\n    return re.sub('\\r',' ', re.sub('\\n', ' ', re.sub(symbols,' ',review)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove newlines and convert all to lowercase\ndf.Reviews = df.Reviews.apply(remove_punctuation)\ndf.Reviews = df.Reviews.apply(lambda n: n.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize Reviews\ndf.Reviews = df.Reviews.apply(nltk.word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Stop Words\nstops = nltk.corpus.stopwords.words('english')\ndef filter_stop_words(tk_review):\n    return [word for word in tk_review if word not in stops]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Reviews=df.Reviews.apply(filter_stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# POS-Tagging\ndf['Reviews2'] = df.Reviews.apply(nltk.pos_tag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatization\nLemming = nltk.stem.WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n'\n\ndef lemmatize_words(tag_review):\n    output = []\n    for w in tag_review:\n        try:\n            lm = Lemming.lemmatize(w[0], pos=penn2morphy(w[1]))\n        except Exception as e:\n            lm = w\n        output.append(lm)\n        \n    return output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Reviews2'] = df.Reviews2.apply(lemmatize_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, with a decent baseline amount of text preprocessing done, time to bin the scores.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bin Scores and view Histogrm\ndef bin_score(row):\n    return 1 if row['Score'] >= 4 else 0\ndf['BinnedScore'] = df.apply(bin_score, axis=1)\ndf.groupby('BinnedScore').size().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we extract Reviews2 and BinnedScore, and do final preprocessing for model-building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract N most common words\ndef n_most_common(reviews, n=100):\n    words = []\n    sentences = reviews.values\n    \n    # Iterate through sentences to extract all unique words\n    for s in sentences:\n        for w in s:\n            if w not in words:\n                words.append(w)\n    \n    # Generate a counter for each word\n    counts = {w: 0 for w in words}\n    \n    # Iterate through all words, incrementing count\n    for sent in sentences:\n        for word in sent:\n            counts[word] += 1\n    \n    # Generate list of \n    words = sorted([(w, counts[w]) for w in words], key= lambda x: x[1], reverse=True)\n    return words[0:n]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MC = n_most_common(df.Reviews2,N_MC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Categorical Variables\nMC = [w[0] for w in MC]\ndef create_categorical_column(dframe, word):\n    def hasword(row):\n        if word in row['Reviews2']:\n            return 1\n        else:\n            return 0\n    dframe[word] = dframe.apply(lambda row: hasword(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF = df.drop(columns=['Unnamed: 0', 'Reviews', 'Score'])\nDF.head()\nDF.to_csv('CleanedDF1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One-Hot Encoding\ndef encode_word(row, word):\n    return 1 if word in row['Reviews2'] else 0\nlenmc = len(MC)\nfor i, word in enumerate(MC):\n    DF[str(word)] = DF.apply(lambda row: encode_word(row, word), axis=1)\n    print(f'{i}/{lenmc}')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\ntrain, test = train_test_split(DF, test_size=0.2)\n\ntrain_y = train.pop('BinnedScore')\ntest_y = test.pop('BinnedScore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With our data prepared, we can start building a model!\n\nNOTE: Have to revise data cleaning to get rid of single-charactar tokens it looks like :/"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Reviews2', inplace=True, axis=1)\ntest.drop('Reviews2', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import ComplementNB, GaussianNB\nmodel = ComplementNB(alpha=1)\nmodel.fit(train, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef pred_cat(prob, CUTOFF_PROB=CUTOFF_PROB):\n    if prob > CUTOFF_PROB:\n        return '1'\n    elif prob < CUTOFF_PROB:\n        return '0'\n    else:\n        return random.choice([0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Predictions!\nprobs = [round(p[1], 4) for p in model.predict_proba(test)]\npredictions = [pred_cat(p) for p in probs]\nobserved = [v for v in test_y.values]\nbenchmark_predictions = [1 for i in test_y.values]\n\nresults = pd.DataFrame([predictions, observed, benchmark_predictions, probs]).transpose()\nresults.rename({0:'Predicted', 1:'Observed', 2:'Benchmark_Predictions', 3:'Probabilities'}, axis=1, inplace=True)\n\nresults = results.apply(lambda x: x.apply(str))\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_right(row):\n    return 1 if row['Predicted'] == row['Observed'] else 0\n\ndef benchmark_right(row):\n    return 1 if row['Benchmark_Predictions'] == row['Observed'] else 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results['model_right'] = results.apply(model_right, axis=1)\nresults['benchmark_right'] = results.apply(benchmark_right, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nAUCstat = roc_auc_score(results['Observed'].apply(float).values.flatten(), results['Probabilities'].apply(float).values.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'model error: {1 - results[\"model_right\"].sum() / len(results)}')\nprint(f'AUC: {AUCstat}')\nprint(f'benchmark error: {1 - results[\"benchmark_right\"].sum() / len(results)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Positives = results[results['Observed'] == '1']\nNegatives = results[results['Observed'] == '0']\n\nsensitivity = len(Positives[Positives['Predicted']=='1']) / len(Positives)\nspecificity = len(Negatives[Negatives['Predicted']=='0']) / len(Negatives)\n\nprint(f'Sensitivty: {round(sensitivity*100, 2)}%')\nprint(f'Specificity: {round(specificity*100, 2)}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Current Baselines: \n\n100 words, 40.75% error, 59.45% Sensitivity, 58.96% Specificity\n\n200 words, 38.25% error, 62.46% Sensitivity, 60.77% Specificity\n\n400 words, 37.02% error, 65.82% Sensitivity, 58.94% Specificity\n\n2000 words, 34.6% error, 0.718 C-stat, 64.28% sensitivity, 67.31% specificity, and ~1hr of runtime because my encoding isn't vectorized"},{"metadata":{},"cell_type":"markdown","source":"ROC Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ROC(rdf):\n    \n    df1 = pd.concat([rdf['Probabilities'], rdf['Observed']], axis=1)\n\n    ss = []\n    sc = []    \n    \n    def populate_ss(ctf):\n        \n        df1['TempPred'] = df1['Probabilities'].apply(lambda p: pred_cat(float(p), CUTOFF_PROB=float(ctf)))\n        \n        Positives = df1[df1['Observed'] == '1']\n        Negatives = df1[df1['Observed'] == '0']\n        \n        sensi = len(Positives[Positives['TempPred'] == '1']) / len(Positives)\n        speci = len(Negatives[Negatives['TempPred'] == '0']) / len(Negatives)\n        \n        ss.append(sensi)\n        sc.append(1-speci)\n\n    cutoffs = np.arange(0, 1, 0.005)\n    for i in cutoffs:\n        populate_ss(i)\n\n    plt.title(\"ROC Chart\")\n    plt.xlabel('1-Specificity')\n    plt.ylabel('Sensitivity')\n    \n    plt.plot([0, 0, 1], [0, 1, 1], c='g', label='Ideal')\n    plt.plot(sc, ss, c='b', label='Observed')\n    plt.plot([0, 1], [0, 1], c='r', label='Benchmark')\n    \n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MC","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'stops' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-787deb22b352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'stops' is not defined"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}